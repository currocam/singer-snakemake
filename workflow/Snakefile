"""
Snakemake workflow for SINGER (github.com/popgenmethods/SINGER). The code below
largely cannibalizes the `singer_parallel` wrapper included with SINGER (by Yun
Deng). The exception is that mutation rate is adjusted on the basis of
accessible sequence per chunk (from a bed file of missing intervals), and an
average recombination rate per chunk is calculated (from a provided
recombination map. Some diagnostic plots are produced at the end.
"""

configfile: "config/example_config.yaml"

__version__ = "0.0.1"
__name__ = "singer-snakemake"

import csv
import msprime
import tskit
import numpy as np
import allel
import matplotlib.pyplot as plt
import yaml
import pickle
import subprocess
from datetime import datetime

def tag(step): 
    return f"[{__name__}::{step}::{str(datetime.now())}]"

# ------ config ------ #

INPUT_DIR = config["input-dir"]
CHROMOSOMES = glob_wildcards(os.path.join(INPUT_DIR, "{chrom}.vcf")).chrom
SINGER_BINARY = config["singer-binary"] 
CHUNK_SIZE = float(config["chunk-size"])
MUTATION_RATE = float(config["mutation-rate"])
MAX_MISSING = float(config["max-missing"])
MCMC_THIN = int(config["mcmc-thin"])
MCMC_SAMPLES = int(config["mcmc-samples"])
MCMC_BURNIN = int(float(config["mcmc-burnin"]) * MCMC_SAMPLES)
MAX_RESUMES = int(config["max-resumes"] * MCMC_SAMPLES)
POLARISED = bool(config["polarised"])
RAND_SEED = int(config["random-seed"])
RECOMBINATION_RATE = float(config["recombination-rate"])
assert MCMC_SAMPLES > 0, "Number of MCMC samples must be nonzero"
assert MCMC_BURNIN < MCMC_SAMPLES, "Burn-in exceeds number of MCMC samples"
np.random.seed(RAND_SEED)
SEED_ARRAY = {n:x for n, x in zip(CHROMOSOMES, np.random.randint(2 ** 20, size=len(CHROMOSOMES)))}

# ------ rules ------ #

rule all:
    input:
        expand("results/{chrom}/site_density.png", chrom=CHROMOSOMES),
        expand("results/{chrom}/diversity-scatter.png", chrom=CHROMOSOMES),
        expand("results/{chrom}/{chrom}.{rep}.trees", chrom=CHROMOSOMES, rep=range(MCMC_SAMPLES)),


checkpoint chunk_chromosomes:
    """
    Chop up chromosome into chunks, adjust mutation rate to reflect missing
    data, calculate statistics in chunks, make some plots of site/missing data density.
    """
    input: 
        vcf = os.path.join(INPUT_DIR, "{chrom}.vcf"),
    output:
        site_density = "results/{chrom}/site_density.png",
        chunks = directory("results/{chrom}/chunks"),
        vcf_stats = "results/{chrom}/{chrom}.vcf.stats.p",
        ratemap = "results/{chrom}/{chrom}.adjusted_mu.p",
        metadata = "results/{chrom}/{chrom}.vcf.samples.p",
    log:
        log = "results/{chrom}/{chrom}.log"
    run:
        logfile = open(log.log, "w")
        np.random.seed(SEED_ARRAY[wildcards.chrom])
        logfile.write(f"{tag(rule)} Random seed {SEED_ARRAY[wildcards.chrom]}\n")
        # read in data, mask, hapmap
        vcf = allel.read_vcf(input.vcf)
        hapmap_file = input.vcf.replace(".vcf", ".hapmap")
        mask_file = input.vcf.replace(".vcf", ".mask.bed")
        meta_file = input.vcf.replace(".vcf", ".meta.csv")
        if not os.path.exists(hapmap_file):
            logfile.write(
                f"{tag(rule)} Did not find {hapmap_file}, using default recombination "
                f"rate of {RECOMBINATION_RATE}\n")
            hapmap = msprime.RateMap(
                position=np.array([0.0, np.max(vcf['variants/POS']) + 1.0]),
                rate=np.array([RECOMBINATION_RATE]),
            )
        else:
            hapmap = msprime.RateMap.read_hapmap(hapmap_file)
        if not os.path.exists(mask_file):
            logfile.write(f"{tag(rule)} Did not find {mask_file}, assuming the entire sequence is accessible\n")
            bedmask = np.empty((0, 2))
        else:
            bedmask = np.loadtxt(mask_file, usecols=[1, 2]).astype(np.int64)
            assert np.max(bedmask) <= hapmap.sequence_length, "Mask position exceeds hapmap length"
        bitmask = np.full(int(hapmap.sequence_length), False)
        for (a, b) in bedmask: bitmask[a:b] = True
        # parse metadata
        if not os.path.exists(meta_file):
            logfile.write(f"{tag(rule)} Did not find {meta_file}, inserting sample names into metadata\n")
            metadata = [{"id":name} for name in vcf["samples"]]
        else:
            meta_file = csv.reader(open(meta_file, "r"))
            metadata = []
            metadata_names = next(meta_file)
            for row in meta_file:
                assert len(row) == len(metadata_names)
                metadata.append({k:v for k, v in zip(metadata_names, row)})
            assert len(metadata) == vcf["samples"].size, "Must have a metadata row for each sample"
        # filter variants to compute stats
        assert not np.all(vcf['calldata/GT'][..., 1] == -1), "VCF must be diploid"
        genotypes, positions = allel.GenotypeArray(vcf['calldata/GT']), vcf['variants/POS']
        assert np.max(positions) <= hapmap.sequence_length, "VCF position exceeds hapmap length"
        counts = genotypes.count_alleles()
        retain = np.logical_and(counts.is_segregating(), counts.is_biallelic())
        masked_sites = np.sum(bitmask[positions - 1])
        if masked_sites:
          logfile.write(
              f"{tag(rule)} WARNING: {masked_sites} sites occur in masked regions, "
              f"but will be visible to SINGER regardless\n"
          )
        retain = np.logical_and(retain, ~bitmask[positions - 1])
        logfile.write(f"{tag(rule)} Calculating statistics with {np.sum(retain)} of {positions.size} variants\n")
        genotypes, positions, counts = genotypes[retain], positions[retain], counts[retain]
        counts[:, 1] = np.sum(counts[:, 1:], axis=1)
        counts = counts[:, :2]
        # divvy chromosome into chunks and compute summary stats per chunk
        lower = np.min(positions) - 1 
        upper = np.max(positions) + 1 
        windows = np.linspace(lower, upper, int((upper - lower) / CHUNK_SIZE) + 1)
        windows = np.unique(np.append(np.append(0, windows), hapmap.sequence_length))
        diversity, _, num_nonmissing, num_sites = allel.windowed_diversity(
            positions,
            counts,
            windows=np.column_stack([windows[:-1] + 1, windows[1:]]).astype(np.int64),
            is_accessible=~bitmask,
            fill=0.0,
        )
        tajima_d, *_ = allel.windowed_tajima_d(
            positions,
            counts,
            windows=np.column_stack([windows[:-1] + 1, windows[1:]]).astype(np.int64),
        )
        folded_afs = allel.sfs_folded(counts, n=2*vcf["samples"].size) / np.sum(~bitmask)
        unfolded_afs = allel.sfs(counts[:, 1], n=2*vcf["samples"].size) / np.sum(~bitmask)
        Ne = 0.25 * allel.sequence_diversity(positions, counts, is_accessible=~bitmask) / MUTATION_RATE
        logfile.write(f"{tag(rule)} Using ballpark Ne estimate of {Ne}\n")
        # average recombination rate within chunks
        rec_rate = np.diff(hapmap.get_cumulative_mass(windows)) / np.diff(windows)
        # filter chunks with too much missingness or zero recombination rate
        num_total = np.diff(windows)
        num_missing = num_total - num_nonmissing
        prop_missing = num_missing / num_total
        prop_snp = num_sites / num_nonmissing
        prop_snp[np.isnan(prop_snp)] = 0.0
        filter = np.logical_and(prop_missing < MAX_MISSING, prop_snp > 0.0)
        filter = np.logical_and(filter, rec_rate > 0.0)
        logfile.write(f"{tag(rule)} Skipping {np.sum(~filter)} (of {filter.size}) chunks with too much missing data\n")
        # plot site density and recombination rate as sanity check
        fig, axs = plt.subplots(3, 1, figsize=(8, 8), sharex=True)
        col = ['black' if x else 'red' for x in filter]
        axs[0].scatter(windows[:-1], prop_missing, s=4, c=col)
        axs[0].set_ylabel("Proportion missing bases")
        axs[1].scatter(windows[:-1], prop_snp, s=4, c=col)
        axs[1].set_ylabel("Proportion variant bases")
        axs[2].scatter(windows[:-1], rec_rate, s=4, c=col)
        axs[2].set_ylabel("Recombination rate")
        axs[2].set_yscale("log")
        fig.supxlabel("Position")
        fig.tight_layout()
        plt.savefig(output.site_density)
        plt.clf()
        # adjust mutation rate to account for missing data in each chunk
        os.makedirs(f"{output.chunks}")
        seeds = np.random.randint(0, 2**10, size=filter.size)
        vcf_prefix = input.vcf.removesuffix(".vcf")
        adj_mu = np.zeros(windows.size - 1)
        for i in np.flatnonzero(filter):
            start, end = windows[i], windows[i + 1]
            adj_mu[i] = (1 - prop_missing[i]) * MUTATION_RATE
            polar = 0.99 if POLARISED else 0.5
            id = f"{i:>06}"
            chunk_params = {
                "thin" : int(MCMC_THIN), "n" : int(MCMC_SAMPLES),
                "Ne" : float(Ne), "m" : float(adj_mu[i]), "input" : str(vcf_prefix), 
                "start" : int(start), "end" : int(end), "polar" : float(polar),
                "r" : float(rec_rate[i]), "seed" : int(seeds[i]),
                "output" : str(f"{output.chunks}/{id}")
            }
            chunk_path = f"{output.chunks}/{id}.yaml"
            yaml.dump(chunk_params, open(chunk_path, "w"), default_flow_style=False)
        # dump adjusted mutation rates and chunk coordinates
        ratemap = msprime.RateMap(
            position=windows, 
            rate=np.array(adj_mu),
        )
        pickle.dump(ratemap, open(output.ratemap, "wb"))
        # dump statistics
        diversity[~filter] = np.nan
        tajima_d[~filter] = np.nan
        vcf_stats = {
            "diversity" : diversity, 
            "tajima_d" : tajima_d, 
            "folded_afs" : folded_afs, 
            "unfolded_afs" : unfolded_afs,
        }
        pickle.dump(vcf_stats, open(output.vcf_stats, "wb"))
        pickle.dump(metadata, open(output.metadata, "wb"))


rule run_singer:
    """
    Run SINGER chunk-by-chunk. Adapted from `singer_master`. 
    Uses the "-resume" flag to resume sampling on error (see https://github.com/popgenmethods/SINGER/issues/13)
    """
    input:
        params = "results/{chrom}/chunks/{id}.yaml"
    output:
        recombs = expand("results/{{chrom}}/chunks/{{id}}_recombs_{rep}.txt", rep=range(MCMC_SAMPLES))
    log:
        out = "results/{chrom}/chunks/{id}.stdout",
        err = "results/{chrom}/chunks/{id}.stderr"
    run:
        logfile = input.params.replace(".yaml", ".log")
        params = yaml.safe_load(open(input.params))
        seed = params.pop("seed")
        invocation = [f"{SINGER_BINARY}"]
        for arg, val in params.items():
            invocation += f"-{arg} {val}".split()
        with open(log.out, "w") as out, open(log.err, "w") as err:
            for attempt in range(MAX_RESUMES):
                args = ["-seed", str(seed + attempt)]
                if os.path.exists(logfile):
                    contents = open(logfile, "r").readlines()
                    if len(contents) > 1 and "rethread" in contents[-2]:
                        # restarting from current iteration sometimes fails,
                        # so restart from previous iteration instead
                        handle = open(logfile, "w")
                        handle.write("".join(contents[:-1]))
                        handle.close()
                        args.append("-resume")
                print(f"{tag(rule)}", " ".join(invocation + args), file=out, flush=True)
                process = subprocess.run(invocation + args, check=False, stdout=out, stderr=err)
                if process.returncode == 0: 
                    break
            print(f"{tag(rule)} SINGER run ended ({process.returncode})", file=out, flush=True)
        assert process.returncode == 0, f"SINGER terminated with error ({process.returncode})"


def merge_chunks_params(wildcards):
    dir = checkpoints.chunk_chromosomes.get(chrom=wildcards.chrom).output.chunks
    out = expand(
        os.path.join(dir, "{id}.yaml"),
        id=glob_wildcards(os.path.join(dir, "{id}.yaml")).id
    )
    return out

def merge_chunks_input(wildcards, regex):
    dir = checkpoints.chunk_chromosomes.get(chrom=wildcards.chrom).output.chunks
    out = expand(
        os.path.join(dir, f"{{id}}_{regex}_{{rep}}.txt"),
        rep=wildcards.rep,
        id=glob_wildcards(os.path.join(dir, "{id}.yaml")).id
    )
    return out

rule merge_chunks:
    """
    Merge chunks into a single tree sequence. Adapted from `convert_long_arg.py`.
    Sets mutations.parent correctly (see https://github.com/popgenmethods/SINGER/issues/11)
    """
    input:
        ratemap = rules.chunk_chromosomes.output.ratemap, 
        metadata = rules.chunk_chromosomes.output.metadata,
        params = merge_chunks_params,
        recombs = lambda w: merge_chunks_input(w, "recombs")
    output:
        trees = "results/{chrom}/{chrom}.{rep}.trees"
    run:
        ratemap = pickle.load(open(input.ratemap, "rb"))
        metadata = pickle.load(open(input.metadata, "rb"))
        tables = tskit.TableCollection(sequence_length=ratemap.sequence_length)
        nodes, edges, individuals = tables.nodes, tables.edges, tables.individuals
        individuals.metadata_schema = tskit.MetadataSchema.permissive_json()
        for x in metadata: individuals.add_row(metadata=x)
        num_nodes, num_samples = 0, 0
        files = zip(input.params, input.recombs)
        for i, (params_file, recomb_file) in enumerate(files):
            node_file = recomb_file.replace("_recombs_", "_nodes_")
            mutation_file = recomb_file.replace("_recombs_", "_muts_")
            branch_file = recomb_file.replace("_recombs_", "_branches_")
            params = yaml.safe_load(open(params_file))
            block_start = params['start']
            # nodes
            node_time = np.loadtxt(node_file)
            num_nodes = nodes.num_rows - num_samples
            min_time = 0
            for t in node_time:
                if t == 0:
                    if i == 0:
                        nodes.add_row(flags=tskit.NODE_IS_SAMPLE, individual=num_samples // 2)
                        num_samples += 1
                else:
                    #assert t >= min_time #gets hit by FP error? TODO
                    t = max(min_time + 1e-7, t)
                    nodes.add_row(time=t)
                    min_time = t
            # edges
            edge_span = np.loadtxt(branch_file)
            edge_span = edge_span[edge_span[:, 2] >= 0, :]
            length = max(edge_span[:, 1])
            parent_indices = np.array(edge_span[:, 2], dtype=np.int32)
            child_indices = np.array(edge_span[:, 3], dtype=np.int32)
            parent_indices[parent_indices >= num_samples] += num_nodes
            child_indices[child_indices >= num_samples] += num_nodes
            edges.append_columns(
                left=edge_span[:, 0] + block_start,
                right=edge_span[:, 1] + block_start,
                parent=parent_indices,
                child=child_indices
            )
            # mutations
            mutations = np.loadtxt(mutation_file)
            num_mutations = mutations.shape[0]
            mut_pos = 0
            for i in range(num_mutations):
                if mutations[i, 0] != mut_pos and mutations[i, 0] < length:
                    tables.sites.add_row(
                        position=mutations[i, 0] + block_start,
                        ancestral_state='0',
                    )
                    mut_pos = mutations[i, 0]
                site_id = tables.sites.num_rows - 1
                mut_node = int(mutations[i, 1])
                if (mut_node < num_samples):
                    tables.mutations.add_row(
                        site=site_id, 
                        node=int(mutations[i, 1]), 
                        derived_state=str(int(mutations[i, 3]))
                    ) 
                else:
                    tables.mutations.add_row(
                        site=site_id, 
                        node=int(mutations[i, 1]) + num_nodes, 
                        derived_state=str(int(mutations[i, 3]))
                    )    
        # rebuild mutations table in time order at each position
        mut_time = tables.nodes.time[tables.mutations.node]
        mut_coord = tables.sites.position[tables.mutations.site]
        mut_order = np.lexsort((-mut_time, mut_coord))
        mut_state = tskit.unpack_strings(
            tables.mutations.derived_state, 
            tables.mutations.derived_state_offset,
        )
        mut_state, mut_state_offset = tskit.pack_strings(np.array(mut_state)[mut_order])
        tables.mutations.set_columns(
            site=tables.mutations.site[mut_order],
            node=tables.mutations.node[mut_order],
            time=np.repeat(tskit.UNKNOWN_TIME, tables.mutations.num_rows),
            derived_state=mut_state,
            derived_state_offset=mut_state_offset,
        )
        tables.sort()
        tables.build_index()
        tables.compute_mutation_parents()
        ts = tables.tree_sequence()
        ts.dump(output.trees)


rule statistics:
    """
    Calculate per-chunk statistics across MCMC iterations.
    """
    input:
        ratemap = rules.chunk_chromosomes.output.ratemap,
        trees = rules.merge_chunks.output.trees
    output:
        stats = "results/{chrom}/{chrom}.{rep,[0-9]+}.stats.p"
    run:
        ratemap = pickle.load(open(input.ratemap, "rb"))
        ts = tskit.load(input.trees)
        diversity = \
            ts.diversity(mode='branch', windows=ratemap.position, span_normalise=True) * MUTATION_RATE
        tajima_d = ts.Tajimas_D(mode='branch', windows=ratemap.position)
        tajima_d[ratemap.rate == 0.0] = np.nan
        folded_afs = \
            ts.allele_frequency_spectrum(mode='branch', span_normalise=True) * MUTATION_RATE * 2
        unfolded_afs = \
            ts.allele_frequency_spectrum(mode='branch', span_normalise=True, polarised=True) * MUTATION_RATE
        stats = {
            "diversity" : diversity, 
            "tajima_d" : tajima_d, 
            "folded_afs" : folded_afs, 
            "unfolded_afs" : unfolded_afs 
        }
        pickle.dump(stats, open(output.stats, "wb"))


def diagnostics_input(wildcards):
    dir = f"results/{wildcards.chrom}"
    out = expand(
        os.path.join(dir, "{chrom}.{rep}.stats.p"),
        chrom=wildcards.chrom,
        rep=range(MCMC_SAMPLES),
    )
    return out

rule diagnostics:
    """
    Make some diagnostic plots per chromosome:
      
    - Per-base site diversity (from VCF) vs branch diversity ("fitted values" from trees) across chunks.
    - Branch diversity (average over chunks) over MCMC iterations.
    - Per-base site and branch diversity over genome position.
    - The above, but with Tajima's D instead of nucleotide diversity
    - Site allele frequency spectrum vs branch allele frequency spectrum (folded and unfolded).
    """
    input:
        ratemap = rules.chunk_chromosomes.output.ratemap,
        vcf_stats = rules.chunk_chromosomes.output.vcf_stats,
        stats = diagnostics_input,
    output:
        diversity_scatter = "results/{chrom}/diversity-scatter.png",
        diversity_trace = "results/{chrom}/diversity-trace.png",
        diversity_skyline = "results/{chrom}/diversity-skyline.png",
        tajima_d_scatter = "results/{chrom}/tajima-d-scatter.png",
        tajima_d_trace = "results/{chrom}/tajima-d-trace.png",
        tajima_d_skyline = "results/{chrom}/tajima-d-skyline.png",
        folded_afs = "results/{chrom}/folded-afs.png",
        unfolded_afs = "results/{chrom}/unfolded-afs.png",
    run:
        ratemap = pickle.load(open(input.ratemap, "rb"))
        windows = ratemap.position
        vcf_stats = pickle.load(open(input.vcf_stats, "rb"))
        site_diversity = vcf_stats['diversity']
        site_tajima_d = vcf_stats['tajima_d']
        site_folded_afs = vcf_stats['folded_afs']
        site_unfolded_afs = vcf_stats['unfolded_afs']
        # aggregate MCMC
        branch_diversity = np.zeros((windows.size - 1, MCMC_SAMPLES))
        branch_tajima_d = np.zeros((windows.size - 1, MCMC_SAMPLES))
        branch_folded_afs = np.zeros((site_folded_afs.size, MCMC_SAMPLES))
        branch_unfolded_afs = np.zeros((site_unfolded_afs.size, MCMC_SAMPLES))
        for i, stats_file in enumerate(input.stats):
            stats = pickle.load(open(stats_file, "rb"))
            branch_diversity[:, i] = stats['diversity']
            branch_tajima_d[:, i] = stats['tajima_d']
            branch_folded_afs[:, i] = stats['folded_afs'][:site_folded_afs.size]
            branch_unfolded_afs[:, i] = stats['unfolded_afs'][:site_unfolded_afs.size]
        # TODO: factorise below
        trace_diversity = np.nanmean(branch_diversity, axis=0)
        trace_tajima_d = np.nanmean(branch_tajima_d, axis=0)
        mean_diversity = np.nanmean(branch_diversity[:, MCMC_BURNIN:], axis=1)
        mean_tajima_d = np.nanmean(branch_tajima_d[:, MCMC_BURNIN:], axis=1)
        mean_folded_afs = np.mean(branch_folded_afs[:, MCMC_BURNIN:], axis=1)
        mean_unfolded_afs = np.mean(branch_unfolded_afs[:, MCMC_BURNIN:], axis=1)
        # scatterplots
        plt.scatter(site_diversity, mean_diversity, c='firebrick', s=8)
        plt.axline((np.nanmean(site_diversity), np.nanmean(site_diversity)), slope=1, color='black')
        plt.xlabel("Site (VCF) diversity per block")
        plt.ylabel("E[diversity] per block")
        plt.savefig(output.diversity_scatter)
        plt.clf()
        plt.scatter(site_tajima_d, mean_tajima_d, c='firebrick', s=8)
        plt.axline((np.nanmean(site_tajima_d), np.nanmean(site_tajima_d)), slope=1, color='black')
        plt.xlabel("Site (VCF) Tajima's D per block")
        plt.ylabel("E[Tajima's D] per block")
        plt.savefig(output.tajima_d_scatter)
        plt.clf()
        # trace
        plt.plot(np.arange(trace_diversity.size), trace_diversity, "-", c='firebrick')
        plt.xlabel("MCMC iteration")
        plt.ylabel("E[diversity]")
        plt.savefig(output.diversity_trace)
        plt.clf()
        plt.plot(np.arange(trace_tajima_d.size), trace_tajima_d, "-", c='firebrick')
        plt.xlabel("MCMC iteration")
        plt.ylabel("E[Tajima's D]")
        plt.savefig(output.tajima_d_trace)
        plt.clf()
        # skyline
        coord = windows[:-1] / 2 + windows[1:] / 2 
        plt.plot(coord, mean_diversity, "-", c='firebrick', label='branch-ARG')
        plt.plot(coord, site_diversity, "-", c='black', label='site-VCF')
        plt.xlabel("Position on chromosome")
        plt.ylabel("Diversity")
        plt.legend()
        plt.savefig(output.diversity_skyline)
        plt.clf()
        plt.plot(coord, mean_tajima_d, "-", c='firebrick', label='branch-ARG')
        plt.plot(coord, site_tajima_d, "-", c='black', label='site-VCF')
        plt.xlabel("Position on chromosome")
        plt.ylabel("Tajima's D")
        plt.legend()
        plt.savefig(output.tajima_d_skyline)
        plt.clf()
        # afs
        plt.scatter(np.arange(site_folded_afs.size), site_folded_afs, c='black', label='site-VCF', s=8)
        plt.scatter(np.arange(site_folded_afs.size), mean_folded_afs, c='firebrick', label='branch-ARG', s=8)
        plt.xlabel("Minor allele frequency")
        plt.ylabel("# of variants / base")
        plt.yscale("log")
        plt.legend()
        plt.savefig(output.folded_afs)
        plt.clf()
        plt.scatter(np.arange(site_unfolded_afs.size), site_unfolded_afs, c='black', label='site-VCF', s=8)
        plt.scatter(np.arange(site_unfolded_afs.size), mean_unfolded_afs, c='firebrick', label='branch-ARG', s=8)
        plt.xlabel("Derive allele frequency")
        plt.ylabel("# of variants / base")
        plt.yscale("log")
        plt.legend()
        plt.savefig(output.unfolded_afs)
        plt.clf()


# ------ lib ------ #
